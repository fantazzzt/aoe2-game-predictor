{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7d44bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ac43448",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataloader(db_file):\n",
    "    conn = sqlite3.connect(db_file)\n",
    "\n",
    "    #Start with the players drawn\n",
    "    ids = pd.read_sql_query('SELECT DISTINCT id FROM players lIMIT 5000', conn) \n",
    "    lst= ids.id.values.tolist()\n",
    "\n",
    "    #Use player ids to grab the games that they played\n",
    "    match_players_df = pd.read_sql_query(f'SELECT * FROM match_players LEFT JOIN matches m on m.id = match_players.match_id WHERE player_id IN ({\", \".join(str(id) for id in lst)})', conn)\n",
    "\n",
    "    return match_players_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37cf934d",
   "metadata": {},
   "source": [
    "This is the massive data frame that we won't touch, but rather grab from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48f8eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mainframe = create_dataloader('bigdata.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5786319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'player_id', 'match_id', 'opening_id', 'civilization', 'victory',\n",
      "       'parser_version', 'time_parsed', 'average_elo', 'map_id', 'time',\n",
      "       'patch_id', 'ladder_id', 'patch_number'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(mainframe.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92ba84f1",
   "metadata": {},
   "source": [
    "This cell is dedicated to finding information about the data. Some of the data points have '-1' as a value for the keys. On the documentation, it states that the default value of victory is -1, along with other keys. We also have over 6000 unique openings from the data. In order to not bias the model, we should scrunch these numbers into only a few general broud openings. There are also instances of 0 elo being the average elo, which cannot be correct given that it is not possible in game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dbf6cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening_id\n",
      "1              1505\n",
      "2               833\n",
      "256            4254\n",
      "257             229\n",
      "258             306\n",
      "              ...  \n",
      "467677440         1\n",
      "467681792         1\n",
      "468713472         2\n",
      "468713984         1\n",
      "4294967295    35434\n",
      "Length: 6101, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# # 5207 uncertain victories\n",
    "# print(mainframe.groupby(by='victory').size())\n",
    "\n",
    "\n",
    "# # Civilization values\n",
    "# print(mainframe.groupby(by='civilization').size())\n",
    "\n",
    "\n",
    "# #Opening ids\n",
    "# print(mainframe.groupby(by='opening_id').size())\n",
    "\n",
    "\n",
    "# #time of match\n",
    "# print(mainframe.groupby(by='time').size())\n",
    "\n",
    "\n",
    "# #Elo\n",
    "# print(mainframe.groupby(by='average_elo').size())\n",
    "\n",
    "\n",
    "# #This is how many unique openings the data observes. 6000+\n",
    "# print(len(pd.unique(mainframe['opening_id'])))\n",
    "\n",
    "#This is what openings are chosen\n",
    "print(mainframe.groupby(by='opening_id').size())\n",
    "\n",
    "# #This is how many unique maps that the data uses: 54\n",
    "# print(len(pd.unique(mainframe['map_id'])))\n",
    "\n",
    "\n",
    "# #This shows if all columns are of the same size: True\n",
    "# if mainframe.apply(len).nunique() == 1:\n",
    "#     print(\"The length of each column is the same.\")\n",
    "# else:\n",
    "#     print(\"The length of each column is different.\")\n",
    "\n",
    "\n",
    "# #This is how much time was spent in the game\n",
    "# print(mainframe.groupby(by='time_parsed').size())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfef84d0",
   "metadata": {},
   "source": [
    "This cell is dedicated to cleaning up the code. In the data, there are rows of -1 values that do not provide any use and can be safely removed, since they don't account for a significant portion of the data. We also want to change the opening_id values so that they represent only a few different openings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c77f4be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(372943, 8)\n"
     ]
    }
   ],
   "source": [
    "#Removes rows that contain a -1 value or have a 0 in average elo\n",
    "altframe = mainframe.loc[(mainframe != -1).all(axis=1) & (mainframe['average_elo'] != 0)]\n",
    "\n",
    "#Removes patch columns, ladder id, and other id elements not relevant to the game\n",
    "altframe = altframe.drop('patch_number', axis=1)\n",
    "altframe = altframe.drop('patch_id', axis=1)\n",
    "altframe = altframe.drop('time_parsed', axis=1)\n",
    "altframe = altframe.drop('match_id', axis=1)\n",
    "altframe = altframe.drop('player_id', axis=1)\n",
    "altframe = altframe.drop('id', axis=1)\n",
    "\n",
    "print(altframe.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ce8e948f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m             loader\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mdata[i] \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mfrom_numpy(x), y)\n\u001b[0;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m train_loader, val_loader\n\u001b[1;32m---> 22\u001b[0m train_loader, val_loader \u001b[39m=\u001b[39m preprocess_data(train_loader, val_loader)\n\u001b[0;32m     23\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAll done with preprocessing and normalizing the data!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[90], line 9\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(train_loader, val_loader)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_data\u001b[39m(train_loader, val_loader):\n\u001b[0;32m      7\u001b[0m     \u001b[39m# Get the mean and standard deviation of the training data\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[1;32m----> 9\u001b[0m     \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m     10\u001b[0m         scaler\u001b[39m.\u001b[39mpartial_fit(x\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m     11\u001b[0m     mean \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mmean_\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "#Preprocess and normalize the data\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess_data(train_loader, val_loader):\n",
    "    # Get the mean and standard deviation of the training data\n",
    "    scaler = StandardScaler()\n",
    "    for x, y in train_loader:\n",
    "        scaler.partial_fit(x.numpy())\n",
    "    mean = scaler.mean_\n",
    "    std = np.sqrt(scaler.var_)\n",
    "    \n",
    "    # Normalize the data using the mean and standard deviation\n",
    "    for loader in [train_loader, val_loader]:\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            x = x.numpy()\n",
    "            x = (x - mean) / std\n",
    "            loader.dataset.data[i] = (torch.from_numpy(x), y)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "train_loader, val_loader = preprocess_data(train_loader, val_loader)\n",
    "print(\"All done with preprocessing and normalizing the data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use CART model from sklearn\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create a CART model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Train the model using the training DataLoader\n",
    "for x, y in train_loader:\n",
    "    model.fit(x.numpy(), y.numpy())\n",
    "    \n",
    "# Evaluate the model on the validation DataLoader\n",
    "correct = 0\n",
    "total = 0\n",
    "for x, y in val_loader:\n",
    "    y_pred = model.predict(x.numpy())\n",
    "    correct += (y_pred == y.numpy()).sum().item()\n",
    "    total += y.shape[0]\n",
    "accuracy = correct / total\n",
    "print(f\"Validation accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e808268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
