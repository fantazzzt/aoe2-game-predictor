{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7d44bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ac43448",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataloader(db_file):\n",
    "    conn = sqlite3.connect(db_file)\n",
    "\n",
    "    #Start with the players drawn\n",
    "    ids = pd.read_sql_query('SELECT DISTINCT id FROM players lIMIT 5000', conn) \n",
    "    lst= ids.id.values.tolist()\n",
    "\n",
    "    #Use player ids to grab the games that they played\n",
    "    match_players_df = pd.read_sql_query(f'SELECT * FROM match_players LEFT JOIN matches m on m.id = match_players.match_id WHERE player_id IN ({\", \".join(str(id) for id in lst)})', conn)\n",
    "    \n",
    "    return match_players_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37cf934d",
   "metadata": {},
   "source": [
    "This is the massive data frame that we won't touch, but rather grab from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48f8eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mainframe = create_dataloader('bigdata.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5786319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'player_id', 'match_id', 'opening_id', 'civilization', 'victory',\n",
      "       'parser_version', 'time_parsed', 'average_elo', 'map_id', 'time',\n",
      "       'patch_id', 'ladder_id', 'patch_number'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(mainframe.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92ba84f1",
   "metadata": {},
   "source": [
    "This cell is dedicated to finding information about the data. Some of the data points have '-1' as a value for the keys. On the documentation, it states that the default value of victory is -1, along with other keys. We also have over 6000 unique openings from the data. In order to not bias the model, we should scrunch these numbers into only a few general broud openings. There are also instances of 0 elo being the average elo, which cannot be correct given that it is not possible in game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dbf6cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5207 uncertain victories\n",
    "# print(mainframe.groupby(by='victory').size())\n",
    "\n",
    "\n",
    "# # Civilization values\n",
    "# print(mainframe.groupby(by='civilization').size())\n",
    "\n",
    "\n",
    "# #Opening ids\n",
    "# print(mainframe.groupby(by='opening_id').size())\n",
    "\n",
    "\n",
    "# #time of match\n",
    "# print(mainframe.groupby(by='time').size())\n",
    "\n",
    "\n",
    "# #Elo\n",
    "# print(mainframe.groupby(by='average_elo').size())\n",
    "\n",
    "\n",
    "# #This is how many unique openings the data observes. 6000+\n",
    "# print(len(pd.unique(mainframe['opening_id'])))\n",
    "\n",
    "# #This is what openings are chosen\n",
    "# print(mainframe.groupby(by='opening_id').size())\n",
    "\n",
    "# #This is how many unique maps that the data uses: 54\n",
    "# print(len(pd.unique(mainframe['map_id'])))\n",
    "\n",
    "\n",
    "# #This shows if all columns are of the same size: True\n",
    "# if mainframe.apply(len).nunique() == 1:\n",
    "#     print(\"The length of each column is the same.\")\n",
    "# else:\n",
    "#     print(\"The length of each column is different.\")\n",
    "\n",
    "\n",
    "# #This is how much time was spent in the game\n",
    "# print(mainframe.groupby(by='time_parsed').size())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfef84d0",
   "metadata": {},
   "source": [
    "This cell is dedicated to cleaning up the code. In the data, there are rows of -1 values that do not provide any use and can be safely removed, since they don't account for a significant portion of the data. We also want to change the opening_id values so that they represent only a few different openings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c77f4be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(372943, 8)\n"
     ]
    }
   ],
   "source": [
    "#Removes rows that contain a -1 value or have a 0 in average elo\n",
    "altframe = mainframe.loc[(mainframe != -1).all(axis=1) & (mainframe['average_elo'] != 0)]\n",
    "\n",
    "#Removes patch columns, ladder id, and other id elements not relevant to the game\n",
    "altframe = altframe.drop('patch_number', axis=1)\n",
    "altframe = altframe.drop('patch_id', axis=1)\n",
    "altframe = altframe.drop('time_parsed', axis=1)\n",
    "altframe = altframe.drop('match_id', axis=1)\n",
    "altframe = altframe.drop('player_id', axis=1)\n",
    "altframe = altframe.drop('id', axis=1)\n",
    "\n",
    "print(altframe.shape)\n",
    "\n",
    "#Crunch down the opening_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce8e948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Preprocess and normalize the data\n",
    "\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# def preprocess_data(train_loader, val_loader):\n",
    "#     # Get the mean and standard deviation of the training data\n",
    "#     scaler = StandardScaler()\n",
    "#     for x, y in train_loader:\n",
    "#         scaler.partial_fit(x.numpy())\n",
    "#     mean = scaler.mean_\n",
    "#     std = np.sqrt(scaler.var_)\n",
    "    \n",
    "#     # Normalize the data using the mean and standard deviation\n",
    "#     for loader in [train_loader, val_loader]:\n",
    "#         for i, (x, y) in enumerate(loader):\n",
    "#             x = x.numpy()\n",
    "#             x = (x - mean) / std\n",
    "#             loader.dataset.data[i] = (torch.from_numpy(x), y)\n",
    "    \n",
    "#     return train_loader, val_loader\n",
    "# train_loader, val_loader = preprocess_data(train_loader, val_loader)\n",
    "# print(\"All done with preprocessing and normalizing the data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Use CART model from sklearn\n",
    "\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# # Create a CART model\n",
    "# model = DecisionTreeClassifier()\n",
    "\n",
    "# # Train the model using the training DataLoader\n",
    "# for x, y in train_loader:\n",
    "#     model.fit(x.numpy(), y.numpy())\n",
    "    \n",
    "# # Evaluate the model on the validation DataLoader\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# for x, y in val_loader:\n",
    "#     y_pred = model.predict(x.numpy())\n",
    "#     correct += (y_pred == y.numpy()).sum().item()\n",
    "#     total += y.shape[0]\n",
    "# accuracy = correct / total\n",
    "# print(f\"Validation accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e808268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
